{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b42f6c",
   "metadata": {},
   "source": [
    "Installing requried packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4755d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\dev\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\dev\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\dev\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\dev\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\dev\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\dev\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for jmespath: [Errno 2] No such file or directory: 'c:\\\\dev\\\\lib\\\\site-packages\\\\jmespath-0.10.0.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7b4cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for jmespath: [Errno 2] No such file or directory: 'c:\\\\dev\\\\lib\\\\site-packages\\\\jmespath-0.10.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_crfsuite in c:\\dev\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: tabulate in c:\\dev\\lib\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\dev\\lib\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\dev\\lib\\site-packages (from sklearn_crfsuite) (0.9.8)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\dev\\lib\\site-packages (from sklearn_crfsuite) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\dev\\lib\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a3dcc",
   "metadata": {},
   "source": [
    "Importing all of the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b253e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96f11bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\millermicd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\millermicd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\millermicd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\millermicd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading the requried data\n",
    "nltk.download('treebank')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02841899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    " \n",
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "173d56a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking what all the labels are for this dataset\n",
    "lab = set([label for pair in tagged_sentence for word, label in pair])       \n",
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9338cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting into training and testing data\n",
    "train, test = train_test_split(tagged_sentence, test_size = .25, random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af5481",
   "metadata": {},
   "source": [
    "we can try and use int() to make values of 0,1 for False and true values. if int(True) = 1 ,int(False) = 0\n",
    "added has_apostrophe to the features to try and make conjunctions better. All of the other features I found were good to use from research into this topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba57fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sentence, index):\n",
    "    return {\n",
    "        'is_first_capital':int(sentence[index][0].isupper()),\n",
    "        'is_first_word': int(index==0),\n",
    "        'is_last_word':int(index==len(sentence)-1),\n",
    "        'is_complete_capital': int(sentence[index].upper()==sentence[index]),\n",
    "        'prev_word':'' if index==0 else sentence[index-1],\n",
    "        'next_word':'' if index==len(sentence)-1 else sentence[index+1],\n",
    "        'is_numeric':int(sentence[index].isdigit()),\n",
    "        'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "        'prefix_1':sentence[index][0],\n",
    "        'prefix_2': sentence[index][:2],\n",
    "        'prefix_3':sentence[index][:3],\n",
    "        'prefix_4':sentence[index][:4],\n",
    "        'suffix_1':sentence[index][-1],\n",
    "        'suffix_2':sentence[index][-2:],\n",
    "        'suffix_3':sentence[index][-3:],\n",
    "        'suffix_4':sentence[index][-4:],\n",
    "        'lem':lemmatizer.lemmatize(sentence[index]),\n",
    "        'has_apostrophe':int(bool(re.match('^(\\w+\\'\\w+)',sentence[index]))),\n",
    "        'word_has_hyphen': 1 if '-' in sentence[index] else 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d39b2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions fro getting the data ready in x, y data format has to parse the pair of (x,y) from a sentence that contains many of them\n",
    "def get_word(pair):\n",
    "    return [word for word, label in pair]\n",
    "def make_data(sentence):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    \n",
    "    for pair in sentence:\n",
    "        parsed_sentence = get_word(pair)\n",
    "        \n",
    "        xdata.append([get_features(parsed_sentence, index) for index in range(len(pair))])\n",
    "        ydata.append([label for word, label in pair])\n",
    "    return xdata, ydata\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be978e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd845822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xtrain , ytrain = make_data(train)\n",
    "xtest , ytest = make_data(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a80e9629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_first_capital': 0,\n",
       "  'is_first_word': 1,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': '',\n",
       "  'next_word': 'It',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': '`',\n",
       "  'prefix_2': '``',\n",
       "  'prefix_3': '``',\n",
       "  'prefix_4': '``',\n",
       "  'suffix_1': '`',\n",
       "  'suffix_2': '``',\n",
       "  'suffix_3': '``',\n",
       "  'suffix_4': '``',\n",
       "  'lem': '``',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': '``',\n",
       "  'next_word': 'could',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'I',\n",
       "  'prefix_2': 'It',\n",
       "  'prefix_3': 'It',\n",
       "  'prefix_4': 'It',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'It',\n",
       "  'suffix_3': 'It',\n",
       "  'suffix_4': 'It',\n",
       "  'lem': 'It',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'It',\n",
       "  'next_word': 'operate',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'c',\n",
       "  'prefix_2': 'co',\n",
       "  'prefix_3': 'cou',\n",
       "  'prefix_4': 'coul',\n",
       "  'suffix_1': 'd',\n",
       "  'suffix_2': 'ld',\n",
       "  'suffix_3': 'uld',\n",
       "  'suffix_4': 'ould',\n",
       "  'lem': 'could',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'could',\n",
       "  'next_word': '*-1',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'o',\n",
       "  'prefix_2': 'op',\n",
       "  'prefix_3': 'ope',\n",
       "  'prefix_4': 'oper',\n",
       "  'suffix_1': 'e',\n",
       "  'suffix_2': 'te',\n",
       "  'suffix_3': 'ate',\n",
       "  'suffix_4': 'rate',\n",
       "  'lem': 'operate',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'operate',\n",
       "  'next_word': 'to',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': '*',\n",
       "  'prefix_2': '*-',\n",
       "  'prefix_3': '*-1',\n",
       "  'prefix_4': '*-1',\n",
       "  'suffix_1': '1',\n",
       "  'suffix_2': '-1',\n",
       "  'suffix_3': '*-1',\n",
       "  'suffix_4': '*-1',\n",
       "  'lem': '*-1',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 1},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': '*-1',\n",
       "  'next_word': 'augment',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 't',\n",
       "  'prefix_2': 'to',\n",
       "  'prefix_3': 'to',\n",
       "  'prefix_4': 'to',\n",
       "  'suffix_1': 'o',\n",
       "  'suffix_2': 'to',\n",
       "  'suffix_3': 'to',\n",
       "  'suffix_4': 'to',\n",
       "  'lem': 'to',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'to',\n",
       "  'next_word': 'our',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'a',\n",
       "  'prefix_2': 'au',\n",
       "  'prefix_3': 'aug',\n",
       "  'prefix_4': 'augm',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'nt',\n",
       "  'suffix_3': 'ent',\n",
       "  'suffix_4': 'ment',\n",
       "  'lem': 'augment',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'augment',\n",
       "  'next_word': 'budget',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'o',\n",
       "  'prefix_2': 'ou',\n",
       "  'prefix_3': 'our',\n",
       "  'prefix_4': 'our',\n",
       "  'suffix_1': 'r',\n",
       "  'suffix_2': 'ur',\n",
       "  'suffix_3': 'our',\n",
       "  'suffix_4': 'our',\n",
       "  'lem': 'our',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'our',\n",
       "  'next_word': ',',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'b',\n",
       "  'prefix_2': 'bu',\n",
       "  'prefix_3': 'bud',\n",
       "  'prefix_4': 'budg',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'et',\n",
       "  'suffix_3': 'get',\n",
       "  'suffix_4': 'dget',\n",
       "  'lem': 'budget',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'budget',\n",
       "  'next_word': \"''\",\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': ',',\n",
       "  'prefix_2': ',',\n",
       "  'prefix_3': ',',\n",
       "  'prefix_4': ',',\n",
       "  'suffix_1': ',',\n",
       "  'suffix_2': ',',\n",
       "  'suffix_3': ',',\n",
       "  'suffix_4': ',',\n",
       "  'lem': ',',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': ',',\n",
       "  'next_word': 'James',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': \"'\",\n",
       "  'prefix_2': \"''\",\n",
       "  'prefix_3': \"''\",\n",
       "  'prefix_4': \"''\",\n",
       "  'suffix_1': \"'\",\n",
       "  'suffix_2': \"''\",\n",
       "  'suffix_3': \"''\",\n",
       "  'suffix_4': \"''\",\n",
       "  'lem': \"''\",\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': \"''\",\n",
       "  'next_word': 'Rill',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'J',\n",
       "  'prefix_2': 'Ja',\n",
       "  'prefix_3': 'Jam',\n",
       "  'prefix_4': 'Jame',\n",
       "  'suffix_1': 's',\n",
       "  'suffix_2': 'es',\n",
       "  'suffix_3': 'mes',\n",
       "  'suffix_4': 'ames',\n",
       "  'lem': 'James',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'James',\n",
       "  'next_word': ',',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'R',\n",
       "  'prefix_2': 'Ri',\n",
       "  'prefix_3': 'Ril',\n",
       "  'prefix_4': 'Rill',\n",
       "  'suffix_1': 'l',\n",
       "  'suffix_2': 'll',\n",
       "  'suffix_3': 'ill',\n",
       "  'suffix_4': 'Rill',\n",
       "  'lem': 'Rill',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'Rill',\n",
       "  'next_word': 'the',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': ',',\n",
       "  'prefix_2': ',',\n",
       "  'prefix_3': ',',\n",
       "  'prefix_4': ',',\n",
       "  'suffix_1': ',',\n",
       "  'suffix_2': ',',\n",
       "  'suffix_3': ',',\n",
       "  'suffix_4': ',',\n",
       "  'lem': ',',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': ',',\n",
       "  'next_word': 'Justice',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 't',\n",
       "  'prefix_2': 'th',\n",
       "  'prefix_3': 'the',\n",
       "  'prefix_4': 'the',\n",
       "  'suffix_1': 'e',\n",
       "  'suffix_2': 'he',\n",
       "  'suffix_3': 'the',\n",
       "  'suffix_4': 'the',\n",
       "  'lem': 'the',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'the',\n",
       "  'next_word': 'Department',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'J',\n",
       "  'prefix_2': 'Ju',\n",
       "  'prefix_3': 'Jus',\n",
       "  'prefix_4': 'Just',\n",
       "  'suffix_1': 'e',\n",
       "  'suffix_2': 'ce',\n",
       "  'suffix_3': 'ice',\n",
       "  'suffix_4': 'tice',\n",
       "  'lem': 'Justice',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'Justice',\n",
       "  'next_word': \"'s\",\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'D',\n",
       "  'prefix_2': 'De',\n",
       "  'prefix_3': 'Dep',\n",
       "  'prefix_4': 'Depa',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'nt',\n",
       "  'suffix_3': 'ent',\n",
       "  'suffix_4': 'ment',\n",
       "  'lem': 'Department',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'Department',\n",
       "  'next_word': 'antitrust',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': \"'\",\n",
       "  'prefix_2': \"'s\",\n",
       "  'prefix_3': \"'s\",\n",
       "  'prefix_4': \"'s\",\n",
       "  'suffix_1': 's',\n",
       "  'suffix_2': \"'s\",\n",
       "  'suffix_3': \"'s\",\n",
       "  'suffix_4': \"'s\",\n",
       "  'lem': \"'s\",\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': \"'s\",\n",
       "  'next_word': 'chief',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'a',\n",
       "  'prefix_2': 'an',\n",
       "  'prefix_3': 'ant',\n",
       "  'prefix_4': 'anti',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'st',\n",
       "  'suffix_3': 'ust',\n",
       "  'suffix_4': 'rust',\n",
       "  'lem': 'antitrust',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'antitrust',\n",
       "  'next_word': ',',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'c',\n",
       "  'prefix_2': 'ch',\n",
       "  'prefix_3': 'chi',\n",
       "  'prefix_4': 'chie',\n",
       "  'suffix_1': 'f',\n",
       "  'suffix_2': 'ef',\n",
       "  'suffix_3': 'ief',\n",
       "  'suffix_4': 'hief',\n",
       "  'lem': 'chief',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'chief',\n",
       "  'next_word': 'said',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': ',',\n",
       "  'prefix_2': ',',\n",
       "  'prefix_3': ',',\n",
       "  'prefix_4': ',',\n",
       "  'suffix_1': ',',\n",
       "  'suffix_2': ',',\n",
       "  'suffix_3': ',',\n",
       "  'suffix_4': ',',\n",
       "  'lem': ',',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': ',',\n",
       "  'next_word': '*T*-2',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 's',\n",
       "  'prefix_2': 'sa',\n",
       "  'prefix_3': 'sai',\n",
       "  'prefix_4': 'said',\n",
       "  'suffix_1': 'd',\n",
       "  'suffix_2': 'id',\n",
       "  'suffix_3': 'aid',\n",
       "  'suffix_4': 'said',\n",
       "  'lem': 'said',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'said',\n",
       "  'next_word': 'in',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 1,\n",
       "  'prefix_1': '*',\n",
       "  'prefix_2': '*T',\n",
       "  'prefix_3': '*T*',\n",
       "  'prefix_4': '*T*-',\n",
       "  'suffix_1': '2',\n",
       "  'suffix_2': '-2',\n",
       "  'suffix_3': '*-2',\n",
       "  'suffix_4': 'T*-2',\n",
       "  'lem': '*T*-2',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 1},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': '*T*-2',\n",
       "  'next_word': 'an',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'i',\n",
       "  'prefix_2': 'in',\n",
       "  'prefix_3': 'in',\n",
       "  'prefix_4': 'in',\n",
       "  'suffix_1': 'n',\n",
       "  'suffix_2': 'in',\n",
       "  'suffix_3': 'in',\n",
       "  'suffix_4': 'in',\n",
       "  'lem': 'in',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'in',\n",
       "  'next_word': 'interview',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'a',\n",
       "  'prefix_2': 'an',\n",
       "  'prefix_3': 'an',\n",
       "  'prefix_4': 'an',\n",
       "  'suffix_1': 'n',\n",
       "  'suffix_2': 'an',\n",
       "  'suffix_3': 'an',\n",
       "  'suffix_4': 'an',\n",
       "  'lem': 'an',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'an',\n",
       "  'next_word': '.',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'i',\n",
       "  'prefix_2': 'in',\n",
       "  'prefix_3': 'int',\n",
       "  'prefix_4': 'inte',\n",
       "  'suffix_1': 'w',\n",
       "  'suffix_2': 'ew',\n",
       "  'suffix_3': 'iew',\n",
       "  'suffix_4': 'view',\n",
       "  'lem': 'interview',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 1,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'interview',\n",
       "  'next_word': '',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': '.',\n",
       "  'prefix_2': '.',\n",
       "  'prefix_3': '.',\n",
       "  'prefix_4': '.',\n",
       "  'suffix_1': '.',\n",
       "  'suffix_2': '.',\n",
       "  'suffix_3': '.',\n",
       "  'suffix_4': '.',\n",
       "  'lem': '.',\n",
       "  'has_apostrophe': 0,\n",
       "  'word_has_hyphen': 0}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4809aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['.', 'PRON', 'VERB', 'X', 'PRT', 'NOUN', 'DET', 'ADJ', 'ADP', 'ADV', 'NUM', 'CONJ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a3a9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(all_possible_transitions=True,\n",
       "                                 keep_tempfiles=None, max_iterations=100),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'algorithm': ['lbfgs', 'l2sgd', 'arow'],\n",
       "                                        'c1': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001C07FB54D60>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001C07FBB6250>},\n",
       "                   scoring=make_scorer(flat_accuracy_score), verbose=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already deteremined that this is a good model fro this problem\n",
    "# trying to determine the best hyperparameters\n",
    "crf = CRF(\n",
    "    \n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.75),\n",
    "    'c2': scipy.stats.expon(scale=0.2),\n",
    "    'algorithm': ['lbfgs','l2sgd','arow']\n",
    "}\n",
    "\n",
    "# using accuracy for figuring out best parameters\n",
    "acc_scorer = make_scorer(metrics.flat_accuracy_score)\n",
    "#,\n",
    "                       # average='weighted', labels=labels)\n",
    "\n",
    "# below searchs for best parameters based on the parameter space\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=100,\n",
    "                        scoring=acc_scorer)\n",
    "rs.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5bab0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'algorithm': 'lbfgs', 'c1': 0.015376338904414984, 'c2': 0.1658155558740988}\n",
      "best CV score: 0.9710130526001003\n"
     ]
    }
   ],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "341c279e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975691400334742"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the best parameters found from above and fitting the model\n",
    "# Then taking the and predicting based on the training data and getting accuracy for the training data\n",
    "crfmodel = CRF(c1 =0.015674981907914637, c2 = 0.06028826022175581)#0.0192736642760659, c2 = 0.1641957022096317\n",
    "\n",
    "crfmodel.fit(xtrain, ytrain)\n",
    "ypred=crfmodel.predict(xtrain)\n",
    "metrics.flat_accuracy_score(ytrain,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eddaca4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758998188548476"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#running the test data through the model and return the accuracy\n",
    "y_pred=crfmodel.predict(xtest)\n",
    "metrics.flat_accuracy_score(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e946542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5008214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .      1.000     0.999     0.999      2924\n",
      "         ADJ      0.918     0.888     0.903      1627\n",
      "         ADP      0.980     0.986     0.983      2484\n",
      "         ADV      0.926     0.925     0.925       771\n",
      "        CONJ      0.997     0.993     0.995       584\n",
      "         DET      0.994     0.992     0.993      2211\n",
      "        NOUN      0.969     0.979     0.974      7345\n",
      "         NUM      0.997     0.989     0.993       912\n",
      "        PRON      0.993     0.998     0.995       664\n",
      "         PRT      0.981     0.974     0.978       809\n",
      "        VERB      0.968     0.966     0.967      3383\n",
      "           X      0.999     0.996     0.998      1680\n",
      "\n",
      "    accuracy                          0.976     25394\n",
      "   macro avg      0.977     0.974     0.975     25394\n",
      "weighted avg      0.976     0.976     0.976     25394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prints a couple metrics based on each label \n",
    "print(metrics.flat_classification_report(\n",
    "    ytest, y_pred, labels=crf.classes_, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "666161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a new function for taking in a sentence and parsing it to get the data to be used for prediction\n",
    "def get_testingfeatures(sentence, index):\n",
    "    sentence = sentence.split(' ')\n",
    "    return {\n",
    "        'is_first_capital':int(sentence[index][0].isupper()),\n",
    "        'is_first_word': int(index==0),\n",
    "        'is_last_word':int(index==len(sentence)-1),\n",
    "        'is_complete_capital': int(sentence[index].upper()==sentence[index]),\n",
    "        'prev_word':'' if index==0 else sentence[index-1],\n",
    "        'next_word':'' if index==len(sentence)-1 else sentence[index+1],\n",
    "        'is_numeric':int(sentence[index].isdigit()),\n",
    "        'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "        'prefix_1':sentence[index][0],\n",
    "        'prefix_2': sentence[index][:2],\n",
    "        'prefix_3':sentence[index][:3],\n",
    "        'prefix_4':sentence[index][:4],\n",
    "        'suffix_1':sentence[index][-1],\n",
    "        'suffix_2':sentence[index][-2:],\n",
    "        'suffix_3':sentence[index][-3:],\n",
    "        'suffix_4':sentence[index][-4:],\n",
    "        'lem':lemmatizer.lemmatize(sentence[index]),\n",
    "        'has_apostrophe':int(bool(re.match('^(\\w+\\'\\w+)',sentence[index]))),\n",
    "        'word_has_hyphen': 1 if '-' in sentence[index] else 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "765cc7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a sentence and then iterates over it and gets all the words and puncutaion and runs it through the above function to get \n",
    "# the appropirate format for the model. \n",
    "# the sentence should be spaced between everything. so spaces inbetween words and punctuation\n",
    "new_sentence = \"My dog ran in the park .\"\n",
    "testingdata = []\n",
    "testingdata.append([get_testingfeatures(new_sentence, index) for index in range(len(new_sentence.split(' ')))])\n",
    "#testingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00c8152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PRON', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', '.']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# runs the new data through the model and returns the results\n",
    "crfmodel.predict(testingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bf768b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ADJ', 'NOUN'), 4.460021),\n",
       " (('VERB', 'PRT'), 3.285621),\n",
       " (('NOUN', 'NOUN'), 3.044629),\n",
       " (('NOUN', 'PRT'), 2.886757),\n",
       " (('NOUN', 'VERB'), 2.834285),\n",
       " (('ADP', 'NOUN'), 2.685314),\n",
       " (('DET', 'NOUN'), 2.291516),\n",
       " (('NUM', 'NOUN'), 2.255194),\n",
       " (('X', 'VERB'), 2.246067),\n",
       " (('PRON', 'VERB'), 2.087049),\n",
       " (('NOUN', 'ADP'), 1.966258),\n",
       " (('ADV', 'VERB'), 1.953971),\n",
       " (('ADV', 'ADJ'), 1.92981),\n",
       " (('VERB', 'NOUN'), 1.871431),\n",
       " (('ADP', 'PRON'), 1.758661),\n",
       " (('NOUN', 'CONJ'), 1.67907),\n",
       " (('NOUN', '.'), 1.670311),\n",
       " (('ADJ', 'ADJ'), 1.48703),\n",
       " (('ADV', 'ADV'), 1.45919),\n",
       " (('CONJ', 'NOUN'), 1.407435)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(crfmodel.transition_features_).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4aa52c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('X', 'DET'), -1.249699),\n",
       " (('X', 'NUM'), -1.307248),\n",
       " (('CONJ', '.'), -1.345672),\n",
       " (('X', 'NOUN'), -1.352653),\n",
       " (('DET', 'DET'), -1.353117),\n",
       " (('ADJ', 'PRON'), -1.394371),\n",
       " (('PRT', '.'), -1.422574),\n",
       " (('X', 'ADJ'), -1.42733),\n",
       " (('ADP', 'PRT'), -1.438229),\n",
       " (('ADV', 'X'), -1.450862),\n",
       " (('PRT', 'PRT'), -1.503432),\n",
       " (('PRON', 'DET'), -1.742286),\n",
       " (('PRON', 'PRT'), -1.767708),\n",
       " (('DET', 'ADP'), -1.894233),\n",
       " (('X', 'PRT'), -1.985065),\n",
       " (('PRT', 'NUM'), -2.065554),\n",
       " (('.', 'PRT'), -2.972698),\n",
       " (('ADP', 'X'), -3.551786),\n",
       " (('CONJ', 'X'), -3.808095),\n",
       " (('DET', 'PRT'), -4.878475)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(crfmodel.transition_features_).most_common()[-20:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
